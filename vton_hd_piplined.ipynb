{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# connect to google drive\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3xCtmTz_JLx","executionInfo":{"status":"ok","timestamp":1716287049801,"user_tz":-210,"elapsed":28391,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"b50e8890-9917-482d-b5d4-17a26483bbfe"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# set directories and global variables"],"metadata":{"id":"V4kWHhES6-RC"}},{"cell_type":"code","source":["uploaded_image_path = '/content/uploaded_image'\n","uploaded_cloth_path = '/content/uploaded_cloth'\n","image_path = '/content/VITON-HD/datasets/test/image'\n","cloth_path = '/content/VITON-HD/datasets/test/cloth'\n","cloth_mask_path = '/content/VITON-HD/datasets/test/cloth-mask'\n","image_parse_path = '/content/VITON-HD/datasets/test/image-parse'\n","openpose_image_path = '/content/VITON-HD/datasets/test/openpose-img'\n","openpose_json_path = '/content/VITON-HD/datasets/test/openpose-json'\n","image_parse_checkpoints = '/content/Self-Correction-Human-Parsing/checkpoints/final.pth'\n","vton_hd_checkpoints = '/content/VITON-HD/checkpoints'"],"metadata":{"id":"zDIkmlR7375j","executionInfo":{"status":"ok","timestamp":1716287060413,"user_tz":-210,"elapsed":683,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!mkdir {uploaded_image_path}\n","!mkdir {uploaded_cloth_path}"],"metadata":{"id":"dsvTGDg_9pH3","executionInfo":{"status":"ok","timestamp":1716287061071,"user_tz":-210,"elapsed":662,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Install Libraries"],"metadata":{"id":"mVJya0i95WkU"}},{"cell_type":"markdown","source":["## mediapipe"],"metadata":{"id":"uR0ogBKsS9Or"}},{"cell_type":"code","source":["! pip install mediapipe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_I5jiHDS_rY","executionInfo":{"status":"ok","timestamp":1716287076060,"user_tz":-210,"elapsed":14991,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"9a4727d5-0714-4434-b769-a7dcdfee5343"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.2.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n","Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.25.2)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe)\n","  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.2.0)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.11.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n","Installing collected packages: protobuf, sounddevice, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mediapipe-0.10.14 protobuf-4.25.3 sounddevice-0.4.6\n"]}]},{"cell_type":"markdown","source":["## image parse model"],"metadata":{"id":"cKVT_XZs-Hx3"}},{"cell_type":"code","metadata":{"id":"P-SD6AWyad5K","executionInfo":{"status":"ok","timestamp":1716287076061,"user_tz":-210,"elapsed":23,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"source":["# !pip install ninja"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"pV22tdU4U1vN","executionInfo":{"status":"ok","timestamp":1716287076061,"user_tz":-210,"elapsed":17,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"source":["# !git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing\n","# %cd Self-Correction-Human-Parsing\n","# !mkdir checkpoints\n","# !mkdir inputs\n","# !mkdir outputs\n","# %cd .."],"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# dataset = 'lip'         #select from ['lip', 'atr', 'pascal']"],"metadata":{"id":"0AwQTz608Z_Z","executionInfo":{"status":"ok","timestamp":1716287076061,"user_tz":-210,"elapsed":16,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# import gdown\n","\n","# if dataset == 'lip':\n","#     url = 'https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH'\n","# elif dataset == 'atr':\n","#     url = 'https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP'\n","# elif dataset == 'pascal':\n","#     url = 'https://drive.google.com/uc?id=1E5YwNKW2VOEayK9mWCS3Kpsxf-3z04ZE'\n","\n","# gdown.download(url, image_parse_checkpoints, quiet=False)"],"metadata":{"id":"gmzmIf2Y8nxW","executionInfo":{"status":"ok","timestamp":1716287076061,"user_tz":-210,"elapsed":14,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXMfrlCDmUsu","executionInfo":{"status":"ok","timestamp":1716287150402,"user_tz":-210,"elapsed":74354,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"1fcd14a7-2df1-4c17-b430-8ce65292ed76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.18-py3-none-any.whl (757 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Collecting thop>=0.1.1 (from ultralytics)\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238 ultralytics-8.2.18\n"]}],"source":["! pip install ultralytics"]},{"cell_type":"markdown","source":["## vton hd model"],"metadata":{"id":"HzjG5TpyGP42"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTKTrY7dcPzn","executionInfo":{"status":"ok","timestamp":1716287152018,"user_tz":-210,"elapsed":1621,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"f5f445ac-4504-4061-b9f0-d49d31e161a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'VITON-HD'...\n","remote: Enumerating objects: 46, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 46 (delta 8), reused 7 (delta 6), pack-reused 31\u001b[K\n","Receiving objects: 100% (46/46), 5.03 MiB | 8.23 MiB/s, done.\n","Resolving deltas: 100% (15/15), done.\n"]}],"source":["! git clone https://github.com/shadow2496/VITON-HD.git"]},{"cell_type":"code","source":["! pip install opencv-python torchgeometry"],"metadata":{"id":"hRWRq4AeFhgu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716287158637,"user_tz":-210,"elapsed":6627,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"b0777482-8956-49cc-fea4-79de71768f85"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Collecting torchgeometry\n","  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchgeometry) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->torchgeometry) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\n","Installing collected packages: torchgeometry\n","Successfully installed torchgeometry-0.1.2\n"]}]},{"cell_type":"code","source":["import gdown\n","\n","gdown.download('https://drive.google.com/uc?id=1RM4OthSM6V4r7kWCu8SbPIPY14Oz8B2u', vton_hd_checkpoints + '/alias_final.pth', quiet=False)\n","gdown.download('https://drive.google.com/uc?id=1MBHBddaAs7sy8W40jzLmNL83AUh035F1', vton_hd_checkpoints + '/gmm_final.pth', quiet=False)\n","gdown.download('https://drive.google.com/uc?id=17U1sooR3mVIbe8a7rZuFIF3kukPchHfZ', vton_hd_checkpoints + '/seg_final.pth', quiet=False)\n"],"metadata":{"id":"e67lwuQUF176","colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"status":"ok","timestamp":1716287184510,"user_tz":-210,"elapsed":25885,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"b6639e41-1e09-42e8-aa06-9bdd871b513b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1RM4OthSM6V4r7kWCu8SbPIPY14Oz8B2u\n","From (redirected): https://drive.google.com/uc?id=1RM4OthSM6V4r7kWCu8SbPIPY14Oz8B2u&confirm=t&uuid=f040b4b8-0f1c-4a53-98c2-0ce02a03245f\n","To: /content/VITON-HD/checkpoints/alias_final.pth\n","100%|██████████| 402M/402M [00:11<00:00, 34.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1MBHBddaAs7sy8W40jzLmNL83AUh035F1\n","To: /content/VITON-HD/checkpoints/gmm_final.pth\n","100%|██████████| 76.2M/76.2M [00:02<00:00, 25.8MB/s]\n","Downloading...\n","From (original): https://drive.google.com/uc?id=17U1sooR3mVIbe8a7rZuFIF3kukPchHfZ\n","From (redirected): https://drive.google.com/uc?id=17U1sooR3mVIbe8a7rZuFIF3kukPchHfZ&confirm=t&uuid=8b8906e6-6ddd-4e3a-be78-30013adcf73f\n","To: /content/VITON-HD/checkpoints/seg_final.pth\n","100%|██████████| 138M/138M [00:02<00:00, 54.8MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/VITON-HD/checkpoints/seg_final.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## cloth segmentation model"],"metadata":{"id":"HgLrMENdjESE"}},{"cell_type":"code","source":["!pip install iglovikov_helper_functions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saoY-nTsjSjs","executionInfo":{"status":"ok","timestamp":1716287194996,"user_tz":-210,"elapsed":10495,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"da8dd34a-e7ca-49a7-d377-6cef577b624f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting iglovikov_helper_functions\n","  Downloading iglovikov_helper_functions-0.0.53-py2.py3-none-any.whl (64 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting addict (from iglovikov_helper_functions)\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Collecting imagecorruptions (from iglovikov_helper_functions)\n","  Downloading imagecorruptions-1.1.2-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (1.4.2)\n","Collecting jpeg4py (from iglovikov_helper_functions)\n","  Downloading jpeg4py-0.1.4.tar.gz (12 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (1.25.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (4.8.0.76)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (2.0.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (9.4.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (0.19.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (1.11.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from iglovikov_helper_functions) (2.2.1+cu121)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->iglovikov_helper_functions) (3.3)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->iglovikov_helper_functions) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->iglovikov_helper_functions) (2024.5.10)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->iglovikov_helper_functions) (1.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->iglovikov_helper_functions) (24.0)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from jpeg4py->iglovikov_helper_functions) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->iglovikov_helper_functions) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->iglovikov_helper_functions) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->iglovikov_helper_functions) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iglovikov_helper_functions) (3.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->iglovikov_helper_functions) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->iglovikov_helper_functions) (12.4.127)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->iglovikov_helper_functions) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->jpeg4py->iglovikov_helper_functions) (2.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->iglovikov_helper_functions) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->iglovikov_helper_functions) (1.3.0)\n","Building wheels for collected packages: jpeg4py\n","  Building wheel for jpeg4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jpeg4py: filename=jpeg4py-0.1.4-py3-none-any.whl size=8424 sha256=3385b865159b536659a780613d38ba413169b2ffb94c498d49d995fe1fa8b2ce\n","  Stored in directory: /root/.cache/pip/wheels/86/c3/0f/348e6cadb3a27435e833d21d91707d653fb159d69f2a867a36\n","Successfully built jpeg4py\n","Installing collected packages: addict, jpeg4py, imagecorruptions, iglovikov_helper_functions\n","Successfully installed addict-2.4.0 iglovikov_helper_functions-0.0.53 imagecorruptions-1.1.2 jpeg4py-0.1.4\n"]}]},{"cell_type":"code","source":["!pip install cloths_segmentation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oztNWERhj5dN","executionInfo":{"status":"ok","timestamp":1716287220917,"user_tz":-210,"elapsed":25930,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"fc63a5a5-ca57-46ec-b4e8-e396c3b4a0d1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting cloths_segmentation\n","  Downloading cloths_segmentation-0.0.2-py2.py3-none-any.whl (9.1 kB)\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from cloths_segmentation) (1.3.1)\n","Requirement already satisfied: iglovikov-helper-functions in /usr/local/lib/python3.10/dist-packages (from cloths_segmentation) (0.0.53)\n","Collecting pytorch-lightning (from cloths_segmentation)\n","  Downloading pytorch_lightning-2.2.4-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-toolbelt (from cloths_segmentation)\n","  Downloading pytorch_toolbelt-0.6.3-py3-none-any.whl (159 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting segmentation-models-pytorch (from cloths_segmentation)\n","  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from cloths_segmentation) (2.2.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cloths_segmentation) (4.66.4)\n","Collecting wandb (from cloths_segmentation)\n","  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->cloths_segmentation) (1.25.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->cloths_segmentation) (1.11.4)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->cloths_segmentation) (0.19.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations->cloths_segmentation) (6.0.1)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations->cloths_segmentation) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->cloths_segmentation) (4.9.0.80)\n","Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (2.4.0)\n","Requirement already satisfied: imagecorruptions in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (1.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (1.4.2)\n","Requirement already satisfied: jpeg4py in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (0.1.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (4.8.0.76)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (2.0.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (9.4.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from iglovikov-helper-functions->cloths_segmentation) (1.2.2)\n","Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->cloths_segmentation) (2023.6.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning->cloths_segmentation)\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->cloths_segmentation) (24.0)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->cloths_segmentation) (4.11.0)\n","Collecting lightning-utilities>=0.8.0 (from pytorch-lightning->cloths_segmentation)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->cloths_segmentation) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->cloths_segmentation) (12.4.127)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from pytorch-toolbelt->cloths_segmentation) (0.17.1+cu121)\n","Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch->cloths_segmentation)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch->cloths_segmentation)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting timm==0.9.2 (from segmentation-models-pytorch->cloths_segmentation)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch->cloths_segmentation)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch->cloths_segmentation) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch->cloths_segmentation) (0.4.3)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->cloths_segmentation) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb->cloths_segmentation)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->cloths_segmentation)\n","  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->cloths_segmentation) (4.2.1)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->cloths_segmentation) (4.25.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->cloths_segmentation) (5.9.5)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->cloths_segmentation) (2.31.0)\n","Collecting sentry-sdk>=1.0.0 (from wandb->cloths_segmentation)\n","  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setproctitle (from wandb->cloths_segmentation)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->cloths_segmentation) (67.7.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->cloths_segmentation) (1.16.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->cloths_segmentation) (3.9.5)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->cloths_segmentation)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->cloths_segmentation) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->cloths_segmentation) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->cloths_segmentation) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->cloths_segmentation) (2024.2.2)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->cloths_segmentation) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->cloths_segmentation) (2024.5.10)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->cloths_segmentation) (1.6.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iglovikov-helper-functions->cloths_segmentation) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->cloths_segmentation) (2.1.5)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from jpeg4py->iglovikov-helper-functions->cloths_segmentation) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->iglovikov-helper-functions->cloths_segmentation) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->iglovikov-helper-functions->cloths_segmentation) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->iglovikov-helper-functions->cloths_segmentation) (2024.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->cloths_segmentation) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->cloths_segmentation) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->cloths_segmentation) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->cloths_segmentation) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->cloths_segmentation) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->cloths_segmentation) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->cloths_segmentation) (4.0.3)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->cloths_segmentation)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->jpeg4py->iglovikov-helper-functions->cloths_segmentation) (2.22)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=c6e4924903460f7200044809c0f11ee8f6705053bd36df5e5afb340ca80c6487\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=894f7f77cb1dcb77f1ebb1f114e7b40aa26cc262915605f67b7a8c5f854e6a30\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: smmap, setproctitle, sentry-sdk, munch, lightning-utilities, docker-pycreds, gitdb, gitpython, wandb, torchmetrics, efficientnet-pytorch, timm, pytorch-toolbelt, pytorch-lightning, pretrainedmodels, segmentation-models-pytorch, cloths_segmentation\n","Successfully installed cloths_segmentation-0.0.2 docker-pycreds-0.4.0 efficientnet-pytorch-0.7.1 gitdb-4.0.11 gitpython-3.1.43 lightning-utilities-0.11.2 munch-4.0.0 pretrainedmodels-0.7.4 pytorch-lightning-2.2.4 pytorch-toolbelt-0.6.3 segmentation-models-pytorch-0.3.3 sentry-sdk-2.2.0 setproctitle-1.3.3 smmap-5.0.1 timm-0.9.2 torchmetrics-1.4.0.post0 wandb-0.17.0\n"]}]},{"cell_type":"markdown","source":["# Import libraries"],"metadata":{"id":"zbyOksPO-QyH"}},{"cell_type":"code","source":["import mediapipe as mp\n","import numpy as np\n","import cv2\n","import copy\n","from mediapipe.framework.formats.landmark_pb2 import NormalizedLandmark\n","import json\n","import os\n","import torch\n","import albumentations as albu\n","from iglovikov_helper_functions.utils.image_utils import load_rgb, pad, unpad\n","from iglovikov_helper_functions.dl.pytorch.utils import tensor_from_rgb_image\n","from cloths_segmentation.pre_trained_models import create_model\n","from ultralytics import YOLO"],"metadata":{"id":"N2G6C3R-5U-k","executionInfo":{"status":"ok","timestamp":1716287242815,"user_tz":-210,"elapsed":21934,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Upload Image and Cloth"],"metadata":{"id":"G_9bYkTi3dq-"}},{"cell_type":"code","source":["current_path = ! pwd"],"metadata":{"id":"DG093T0d_mzQ","executionInfo":{"status":"ok","timestamp":1716287242816,"user_tz":-210,"elapsed":44,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["! rm -rf /content/uploaded_image/*"],"metadata":{"id":"n1dcsztpAlVp","executionInfo":{"status":"ok","timestamp":1716287242817,"user_tz":-210,"elapsed":11,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"id":"gA3pSTdu3Kmp","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1716287275669,"user_tz":-210,"elapsed":32862,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"eacf439e-209f-4178-a817-2ca81bb8c966"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/uploaded_image\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-f1e73fff-4b07-404b-ad5c-10ca397e1e9e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f1e73fff-4b07-404b-ad5c-10ca397e1e9e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 00891_00.jpg to 00891_00.jpg\n","Saving 03615_00.jpg to 03615_00.jpg\n","Saving 07445_00.jpg to 07445_00.jpg\n","Saving 07573_00.jpg to 07573_00.jpg\n","Saving 08909_00.jpg to 08909_00.jpg\n","Saving 10549_00.jpg to 10549_00.jpg\n","/content\n"]}],"source":["#Please select images which you want to upload\n","%cd {uploaded_image_path}\n","from google.colab import files\n","uploaded = files.upload()\n","%cd {current_path[0]}"]},{"cell_type":"code","source":["! rm -rf /content/uploaded_cloth/*"],"metadata":{"id":"U1WGF-GvAsU5","executionInfo":{"status":"ok","timestamp":1716287276222,"user_tz":-210,"elapsed":558,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#Please select images which you want to upload\n","%cd {uploaded_cloth_path}\n","from google.colab import files\n","uploaded = files.upload()\n","%cd {current_path[0]}"],"metadata":{"id":"_tO7wl9L_Dev","colab":{"base_uri":"https://localhost:8080/","height":246},"executionInfo":{"status":"ok","timestamp":1716287303893,"user_tz":-210,"elapsed":27684,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"75980e38-06ac-4dc2-97f5-acc4e25f0a10"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/uploaded_cloth\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-ebef1477-81d6-47ff-a41b-49f69f180407\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-ebef1477-81d6-47ff-a41b-49f69f180407\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 06802_00.jpg to 06802_00.jpg\n","Saving 07429_00.jpg to 07429_00.jpg\n","Saving 08348_00.jpg to 08348_00.jpg\n","Saving 09933_00.jpg to 09933_00.jpg\n","Saving 11351_00.jpg to 11351_00.jpg\n","/content\n"]}]},{"cell_type":"markdown","source":["## resize image and cloth"],"metadata":{"id":"GjjgYwAW5LK1"}},{"cell_type":"code","source":["! rm -rf /content/VITON-HD/datasets/test/image/*"],"metadata":{"id":"veXhCO3LC8lh","executionInfo":{"status":"ok","timestamp":1716287304378,"user_tz":-210,"elapsed":506,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["! rm -rf /content/VITON-HD/datasets/test/cloth/*"],"metadata":{"id":"bQC0-FiUC04h","executionInfo":{"status":"ok","timestamp":1716287304378,"user_tz":-210,"elapsed":7,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["! mkdir -p {image_path}\n","image_files = [f for f in os.listdir(uploaded_image_path) if os.path.isfile(os.path.join(uploaded_image_path, f))]\n","for image_name in image_files:\n","    image = cv2.imread(uploaded_image_path + '/' + image_name)\n","    resized_image = cv2.resize(image, (768, 1024))\n","    cv2.imwrite(image_path + '/' + image_name.split('.')[0] + '.jpg', resized_image)\n","\n","! mkdir {cloth_path}\n","cloth_files = [f for f in os.listdir(uploaded_cloth_path) if os.path.isfile(os.path.join(uploaded_cloth_path, f))]\n","for image_name in cloth_files:\n","    image = cv2.imread(uploaded_cloth_path + '/' + image_name)\n","    resized_image = cv2.resize(image, (768, 1024))\n","    cv2.imwrite(cloth_path + '/' + image_name.split('.')[0] + '.jpg', resized_image)"],"metadata":{"id":"z1ZgGLgK5Kgy","executionInfo":{"status":"ok","timestamp":1716287305046,"user_tz":-210,"elapsed":674,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["# Create Pose Estimation"],"metadata":{"id":"LMeFHm0D4JX4"}},{"cell_type":"markdown","source":["## function for draw body_25 format"],"metadata":{"id":"cUd9QUwh4kRj"}},{"cell_type":"code","source":["def draw_body_25(output_name, image, landmarks, pairs, thickness=2, visibility=0.2):\n","    refactored_landmarks = copy.deepcopy(landmarks)\n","\n","    # get the cordinate of pixels\n","    image_height, image_width, _ = image.shape\n","    for index in range(len(refactored_landmarks)):\n","        refactored_landmarks[index].x = int(refactored_landmarks[index].x * image_width)\n","        refactored_landmarks[index].y = int(refactored_landmarks[index].y * image_height)\n","\n","    # draw lines\n","    for pair, color in pairs:\n","        if (refactored_landmarks[pair[0]].visibility > visibility and refactored_landmarks[pair[1]].visibility > visibility):\n","            cv2.line(image,\n","                    (int(refactored_landmarks[pair[0]].x), int(refactored_landmarks[pair[0]].y)),\n","                    (int(refactored_landmarks[pair[1]].x), int(refactored_landmarks[pair[1]].y)),\n","                    color, thickness)\n","\n","    # write image\n","    cv2.imwrite(output_name, cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"],"metadata":{"id":"Wy8V6yWa4cRn","executionInfo":{"status":"ok","timestamp":1716287305047,"user_tz":-210,"elapsed":9,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["## function for create json file with body_25 format"],"metadata":{"id":"sYydhLDX4xY0"}},{"cell_type":"code","source":["def json_body_25(output_name, image_width, image_height, landmarks, orders, visibility=0.2):\n","\n","    json_data = {\"version\": 1.3,\n","                \"people\": [\n","                        {\"person_id\": [-1],\n","                        \"pose_keypoints_2d\": [],\n","                        \"face_keypoints_2d\": [],\n","                        \"hand_left_keypoints_2d\": [],\n","                        \"hand_right_keypoints_2d\": [],\n","                        \"pose_keypoints_3d\": [],\n","                        \"face_keypoints_3d\": [],\n","                        \"hand_left_keypoints_3d\": [],\n","                        \"hand_right_keypoints_3d\": []\n","                        }\n","                    ]\n","                }\n","\n","    refactored_landmarks = copy.deepcopy(landmarks)\n","\n","    # get the cordinate of pixels\n","    for index in range(len(refactored_landmarks)):\n","        refactored_landmarks[index].x = refactored_landmarks[index].x * image_width\n","        refactored_landmarks[index].y = refactored_landmarks[index].y * image_height\n","\n","    # add json data\n","    for index in orders:\n","        if refactored_landmarks[index].visibility > visibility:\n","            json_data[\"people\"][0]['pose_keypoints_2d'].append(refactored_landmarks[index].x)\n","            json_data[\"people\"][0]['pose_keypoints_2d'].append(refactored_landmarks[index].y)\n","            json_data[\"people\"][0]['pose_keypoints_2d'].append(refactored_landmarks[index].visibility)\n","        else:\n","            json_data[\"people\"][0]['pose_keypoints_2d'].append(0)\n","            json_data[\"people\"][0]['pose_keypoints_2d'].append(0)\n","            json_data[\"people\"][0]['pose_keypoints_2d'].append(0)\n","    json_data[\"people\"][0]['pose_keypoints_2d'].append(0)\n","    json_data[\"people\"][0]['pose_keypoints_2d'].append(0)\n","    json_data[\"people\"][0]['pose_keypoints_2d'].append(0)\n","\n","    # write json file\n","    with open(output_name, \"w\") as outfile:\n","        outfile.write(json.dumps(json_data, indent=4))"],"metadata":{"id":"nssQLmr-4quC","executionInfo":{"status":"ok","timestamp":1716287305048,"user_tz":-210,"elapsed":9,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## add neck and waist cordinate to new landmarks and pass to draw function"],"metadata":{"id":"OgsMcQ8T4-fF"}},{"cell_type":"code","source":["! mkdir {openpose_image_path}\n","! mkdir {openpose_json_path}\n","\n","mp_pose = mp.solutions.pose\n","\n","pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, enable_segmentation=True, min_detection_confidence=0.5)\n","\n","# images path\n","image_files = [f for f in os.listdir(image_path) if os.path.isfile(os.path.join(image_path, f))]\n","\n","# read images\n","for idx, file in enumerate(image_files):\n","  image = cv2.imread(image_path + '/' + file)\n","  image_height, image_width, _ = image.shape\n","\n","  # Convert the BGR image to RGB before processing.\n","  results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","\n","  if not results.pose_landmarks:\n","    continue\n","\n","  ### draw body 25 ###\n","\n","  landmarks = copy.deepcopy(results.pose_landmarks.landmark)\n","\n","  # neck cordinate\n","  neck = NormalizedLandmark()\n","  neck.x = (landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x + landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x)/2\n","  neck.y = (landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y + landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y)/2\n","  neck.visibility = 1\n","\n","  # waist cordinate\n","  waist = NormalizedLandmark()\n","  waist.x = (landmarks[mp_pose.PoseLandmark.RIGHT_HIP].x + landmarks[mp_pose.PoseLandmark.LEFT_HIP].x)/2\n","  waist.y = (landmarks[mp_pose.PoseLandmark.RIGHT_HIP].y + landmarks[mp_pose.PoseLandmark.LEFT_HIP].y)/2\n","  waist.visibility = 1\n","\n","  # append new cordinates to landmarks\n","  landmarks.append(neck)\n","  mp_pose.PoseLandmark.NECK = len(landmarks) - 1\n","  landmarks.append(waist)\n","  mp_pose.PoseLandmark.WAIST = len(landmarks) - 1\n","\n","  # create pairs\n","  pairs = []\n","  pairs.append(((mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.RIGHT_EYE), (255, 0, 170)))\n","  pairs.append(((mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.LEFT_EYE), (170, 0, 255)))\n","  pairs.append(((mp_pose.PoseLandmark.RIGHT_EAR, mp_pose.PoseLandmark.RIGHT_EYE), (255, 0, 255)))\n","  pairs.append(((mp_pose.PoseLandmark.LEFT_EAR, mp_pose.PoseLandmark.LEFT_EYE), (85, 0, 255)))\n","  pairs.append(((mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.NECK), (255, 0, 85)))\n","  pairs.append(((mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.NECK), (170, 255, 0)))\n","  pairs.append(((mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.NECK), (255, 85, 0)))\n","  pairs.append(((mp_pose.PoseLandmark.NECK, mp_pose.PoseLandmark.WAIST), (255, 0, 0)))\n","  pairs.append(((mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_SHOULDER), (85, 255, 0)))\n","  pairs.append(((mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_SHOULDER), (255, 170, 0)))\n","  pairs.append(((mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST), (0, 255, 0)))\n","  pairs.append(((mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST), (255, 255, 0)))\n","  pairs.append(((mp_pose.PoseLandmark.WAIST, mp_pose.PoseLandmark.LEFT_HIP), (0, 170, 255)))\n","  pairs.append(((mp_pose.PoseLandmark.WAIST, mp_pose.PoseLandmark.RIGHT_HIP), (0, 255, 85)))\n","  pairs.append(((mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_HIP), (0, 255, 170)))\n","  pairs.append(((mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_HIP), (0, 85, 255)))\n","  pairs.append(((mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE), (0, 0, 255)))\n","  pairs.append(((mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE), (0, 255, 255)))\n","\n","  # draw\n","  back_ground = np.zeros((image_height, image_width, 3), dtype=np.uint8)\n","  draw_body_25(openpose_image_path + '/' + file.split('.')[0] + '_rendered.png', back_ground, landmarks, pairs, 10)\n","\n","  # json file\n","  orders = [mp_pose.PoseLandmark.NOSE,\n","            mp_pose.PoseLandmark.NECK,\n","            mp_pose.PoseLandmark.RIGHT_SHOULDER,\n","            mp_pose.PoseLandmark.RIGHT_ELBOW,\n","            mp_pose.PoseLandmark.RIGHT_WRIST,\n","            mp_pose.PoseLandmark.LEFT_SHOULDER,\n","            mp_pose.PoseLandmark.LEFT_ELBOW,\n","            mp_pose.PoseLandmark.LEFT_WRIST,\n","            mp_pose.PoseLandmark.WAIST,\n","            mp_pose.PoseLandmark.RIGHT_HIP,\n","            mp_pose.PoseLandmark.RIGHT_KNEE,\n","            mp_pose.PoseLandmark.RIGHT_ANKLE,\n","            mp_pose.PoseLandmark.LEFT_HIP,\n","            mp_pose.PoseLandmark.LEFT_KNEE,\n","            mp_pose.PoseLandmark.LEFT_ANKLE,\n","            mp_pose.PoseLandmark.RIGHT_EYE,\n","            mp_pose.PoseLandmark.LEFT_EYE,\n","            mp_pose.PoseLandmark.RIGHT_EAR,\n","            mp_pose.PoseLandmark.LEFT_EAR,\n","            mp_pose.PoseLandmark.LEFT_FOOT_INDEX,\n","            mp_pose.PoseLandmark.LEFT_FOOT_INDEX,\n","            mp_pose.PoseLandmark.LEFT_HEEL,\n","            mp_pose.PoseLandmark.RIGHT_FOOT_INDEX,\n","            mp_pose.PoseLandmark.RIGHT_FOOT_INDEX,\n","            mp_pose.PoseLandmark.RIGHT_HEEL,\n","            ]\n","  json_body_25(openpose_json_path + '/' + file.split('.')[0] + '_keypoints.json', image_width, image_height, landmarks, orders)\n","\n","  # crop person from image\n","  croped_image = image.copy()\n","  condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.5\n","  bg_image = np.zeros(image.shape, dtype=np.uint8)\n","  bg_image[:] = (255, 255, 255)\n","  croped_image = np.where(condition, croped_image, bg_image)\n","  cv2.imwrite(image_path + '/' + file, croped_image)\n","\n","  print(f'{file} done')"],"metadata":{"id":"x9odwTgT4_XW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716287307367,"user_tz":-210,"elapsed":2327,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"cfd1d82d-575b-457d-8bc4-6950f4daaff9"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading model to /usr/local/lib/python3.10/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"]},{"output_type":"stream","name":"stdout","text":["08909_00.jpg done\n","07573_00.jpg done\n","00891_00.jpg done\n","07445_00.jpg done\n","03615_00.jpg done\n","10549_00.jpg done\n"]}]},{"cell_type":"markdown","source":["# Create Image Parse"],"metadata":{"id":"Xx1A8QgY8c3A"}},{"cell_type":"markdown","source":["colors:\n","\n","- 0, 0, 254 -> hair -> 2\n","- 0, 51, 85 -> neck -> 10\n","- 0, 85, 254 -> upper body -> 5\n","- 85, 85, 0 -> lower body -> 9\n","- 220, 169, 51 -> left hand (right hand from our pov) -> 14\n","- 254, 0, 0 -> face -> 13\n","- 254, 254, 0 -> right hand (left hand from our pov) -> 15\n","- 0, 128, 0 -> skirt -> 9\n","- 0, 0, 0 -> back ground"],"metadata":{"id":"xAuIaVZZuKbn"}},{"cell_type":"code","source":["# !cd /content/Self-Correction-Human-Parsing && python3 simple_extractor.py --dataset {dataset} --model-restore {image_parse_checkpoints} --input-dir {image_path} --output-dir {image_parse_path}"],"metadata":{"id":"n5qFDdyx8raH","executionInfo":{"status":"ok","timestamp":1716287307367,"user_tz":-210,"elapsed":8,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["model = YOLO('/content/drive/MyDrive/ai_projects/vton/image_parsing/yolov8x/training/weights/best.pt')"],"metadata":{"id":"cpzJDcBtr8xA","executionInfo":{"status":"ok","timestamp":1716287319052,"user_tz":-210,"elapsed":11692,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model.names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKL0RSbPy5jL","executionInfo":{"status":"ok","timestamp":1716287319052,"user_tz":-210,"elapsed":8,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"fb560a4c-00fe-4a91-82cd-2dca1eeb8334"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'hair',\n"," 1: 'neck',\n"," 2: 'upper_body',\n"," 3: 'lower_body',\n"," 4: 'left_hand',\n"," 5: 'face',\n"," 6: 'right_hand',\n"," 7: 'skirt'}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["from PIL import Image\n","\n","! mkdir {image_parse_path}\n","\n","# colors = [[0, 0, 254], [0, 51, 85], [0, 85, 254], [85, 85, 0], [220, 169, 51], [254, 0, 0], [254, 254, 0], [0, 128, 0]]\n","colors = [2, 10, 5, 9, 14, 13, 15, 9]\n","\n","image_files = [f for f in os.listdir(image_path) if os.path.isfile(os.path.join(image_path, f))]\n","\n","for image_name in image_files:\n","    image = cv2.imread(image_path + '/' + image_name)\n","    results = model.predict(source=image.copy(), save=True, save_txt=False, stream=True)\n","\n","    image_parse = np.zeros(image.shape[:2])\n","    for result in results:\n","        # getting the class labels\n","        torch_cls = result.boxes.cls\n","        # masks in numpy array\n","        masks_numpy = result.masks.data.cpu().numpy()\n","        # add mask to image pars\n","        for index, cls in enumerate(torch_cls):\n","            image_parse[cv2.resize(masks_numpy[index], (image_parse.shape[1], image_parse.shape[0])) == 1] = colors[int(cls)]\n","        cv2.imwrite(image_parse_path + '/' + image_name.split('.')[0] + '.png', image_parse)\n","        # image_parse_rgb = cv2.cvtColor(image_parse.astype('uint8'), cv2.COLOR_BGR2RGB)\n","        # Convert the NumPy array to a PIL Image\n","        # pil_image = Image.fromarray(image_parse.astype(np.uint8))\n","        # image_p = pil_image.convert(\"P\", palette=Image.ADAPTIVE, colors=256)\n","        # Save the image to a file\n","        # image_p.save(image_parse_path + '/' + image_name.split('.')[0] + '.png')"],"metadata":{"id":"1ZqcMHLAyNeH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716287323875,"user_tz":-210,"elapsed":4829,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"7c0d1bb1-5498-4f46-d324-37de68439746"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 1280x960 1 hair, 1 neck, 1 upper_body, 1 left_hand, 1 face, 2 right_hands, 1 skirt, 186.6ms\n","Speed: 10.6ms preprocess, 186.6ms inference, 680.0ms postprocess per image at shape (1, 3, 1280, 960)\n","Results saved to \u001b[1mruns/segment/predict\u001b[0m\n","\n","0: 1280x960 1 hair, 1 neck, 1 upper_body, 1 lower_body, 1 left_hand, 1 face, 1 right_hand, 183.4ms\n","Speed: 14.8ms preprocess, 183.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1280, 960)\n","Results saved to \u001b[1mruns/segment/predict\u001b[0m\n","\n","0: 1280x960 1 hair, 1 neck, 1 upper_body, 1 lower_body, 1 left_hand, 1 face, 181.9ms\n","Speed: 13.3ms preprocess, 181.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1280, 960)\n","Results saved to \u001b[1mruns/segment/predict\u001b[0m\n","\n","0: 1280x960 1 hair, 1 neck, 1 upper_body, 2 left_hands, 1 face, 2 right_hands, 1 skirt, 184.3ms\n","Speed: 15.8ms preprocess, 184.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1280, 960)\n","Results saved to \u001b[1mruns/segment/predict\u001b[0m\n","\n","0: 1280x960 1 hair, 1 neck, 1 upper_body, 1 lower_body, 1 left_hand, 1 face, 1 right_hand, 184.4ms\n","Speed: 9.0ms preprocess, 184.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1280, 960)\n","Results saved to \u001b[1mruns/segment/predict\u001b[0m\n","\n","0: 1280x960 2 hairs, 1 neck, 1 upper_body, 1 lower_body, 1 left_hand, 1 face, 1 right_hand, 186.1ms\n","Speed: 9.4ms preprocess, 186.1ms inference, 7.4ms postprocess per image at shape (1, 3, 1280, 960)\n","Results saved to \u001b[1mruns/segment/predict\u001b[0m\n"]}]},{"cell_type":"markdown","source":["# Create Cloth Mask"],"metadata":{"id":"JWFo6Dmn-dtu"}},{"cell_type":"code","source":["cloth_segmentation_model = create_model(\"Unet_2020-10-30\")"],"metadata":{"id":"67NMeJp5-okM","executionInfo":{"status":"ok","timestamp":1716287325945,"user_tz":-210,"elapsed":2083,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"42c33e3a-530f-4032-a2ba-7f48e64a1042"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/ternaus/cloths_segmentation/releases/download/0.0.1/weights.zip\" to /root/.cache/torch/hub/checkpoints/weights.zip\n","100%|██████████| 47.0M/47.0M [00:00<00:00, 221MB/s]\n","/usr/local/lib/python3.10/dist-packages/torch/hub.py:682: UserWarning: Falling back to the old format < 1.6. This support will be deprecated in favor of default zipfile format introduced in 1.6. Please redo torch.save() to save it in the new zipfile format.\n","  warnings.warn('Falling back to the old format < 1.6. This support will be '\n"]}]},{"cell_type":"code","source":["cloth_segmentation_model.eval();"],"metadata":{"id":"M8duTF4dkVTY","executionInfo":{"status":"ok","timestamp":1716287325947,"user_tz":-210,"elapsed":20,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["! mkdir {cloth_mask_path}\n","\n","cloth_files = [f for f in os.listdir(cloth_path) if os.path.isfile(os.path.join(cloth_path, f))]\n","\n","for cloth_name in cloth_files:\n","    image = load_rgb(cloth_path + '/' + cloth_name)\n","\n","    transform = albu.Compose([albu.Normalize(p=1)], p=1)\n","\n","    padded_image, pads = pad(image, factor=32, border=cv2.BORDER_CONSTANT)\n","\n","    x = transform(image=padded_image)[\"image\"]\n","    x = torch.unsqueeze(tensor_from_rgb_image(x), 0)\n","\n","    with torch.no_grad():\n","        prediction = cloth_segmentation_model(x)[0][0]\n","\n","    mask = (prediction > 0).cpu().numpy().astype(np.uint8)\n","\n","    mask = unpad(mask, pads)\n","\n","    # write mask\n","    cv2.imwrite(cloth_mask_path + '/' + cloth_name , mask*255)\n","\n","    # crop image\n","    mask_3d = np.repeat(mask[:, :, np.newaxis], 3, axis=2)\n","    crop_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)*mask_3d\n","    # cv2.imwrite(cloth_path + '/' + cloth_name, cv2.cvtColor(image, cv2.COLOR_BGR2RGB)*mask_3d)\n","\n","    # white background\n","    back_ground = np.ones(image.shape)*255\n","    mask[mask == 1] = 2\n","    mask[mask == 0] = 1\n","    mask[mask == 2] = 0\n","    mask_3d = np.repeat(mask[:, :, np.newaxis], 3, axis=2)\n","    crop_back_ground = back_ground*mask_3d\n","\n","    # write crop image with white background\n","    cv2.imwrite(cloth_path + '/' + cloth_name, crop_back_ground + crop_image)\n","\n","    print(cloth_name, 'done')"],"metadata":{"id":"4vxGRl3wlWyJ","executionInfo":{"status":"ok","timestamp":1716287344128,"user_tz":-210,"elapsed":18198,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"10a17e13-4ea5-4d8e-e384-b1a2259568d0"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["06802_00.jpg done\n","08348_00.jpg done\n","09933_00.jpg done\n","11351_00.jpg done\n","07429_00.jpg done\n"]}]},{"cell_type":"markdown","source":["# Create Text Pairs File"],"metadata":{"id":"EMw_qLSVIjOU"}},{"cell_type":"code","source":["image_files = [f for f in os.listdir(image_path) if os.path.isfile(os.path.join(image_path, f))]\n","cloth_files = [f for f in os.listdir(cloth_path) if os.path.isfile(os.path.join(cloth_path, f))]\n","\n","! rm /content/VITON-HD/datasets/test_pairs.txt\n","\n","with open('/content/VITON-HD/datasets/test_pairs.txt', 'w') as test_pairs:\n","    for image_name in image_files:\n","        for cloth_name in cloth_files:\n","            test_pairs.write(image_name + ' ' + cloth_name + '\\n')"],"metadata":{"id":"fBYX3WE-Iq5K","executionInfo":{"status":"ok","timestamp":1716287344128,"user_tz":-210,"elapsed":27,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"20adbeeb-dd51-4752-ce86-80ec8e9b080c"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/VITON-HD/datasets/test_pairs.txt': No such file or directory\n"]}]},{"cell_type":"markdown","source":["# vton-hd inference"],"metadata":{"id":"Ve8siH6m_XYS"}},{"cell_type":"code","source":["! rm -rf /content/VITON-HD/results/first_test/*"],"metadata":{"id":"3kSmuL-kMNyk","executionInfo":{"status":"ok","timestamp":1716287344130,"user_tz":-210,"elapsed":19,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["! cd /content/VITON-HD && python test.py --name first_test"],"metadata":{"id":"OfKOQr8F_cwQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716287377746,"user_tz":-210,"elapsed":33631,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"c4529e86-6fbd-4989-c6d3-0bd77d932fc2"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(name='first_test', batch_size=1, workers=1, load_height=1024, load_width=768, shuffle=False, dataset_dir='./datasets/', dataset_mode='test', dataset_list='test_pairs.txt', checkpoint_dir='./checkpoints/', save_dir='./results/', display_freq=1, seg_checkpoint='seg_final.pth', gmm_checkpoint='gmm_final.pth', alias_checkpoint='alias_final.pth', semantic_nc=13, init_type='xavier', init_variance=0.02, grid_size=5, norm_G='spectralaliasinstance', ngf=64, num_upsampling_layers='most')\n","Network [SegGenerator] was created. Total number of parameters: 34.5 million. To see the architecture, do print(network).\n","Network [ALIASGenerator] was created. Total number of parameters: 100.5 million. To see the architecture, do print(network).\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\n","step: 1\n","step: 2\n","step: 3\n","step: 4\n","step: 5\n","step: 6\n","step: 7\n","step: 8\n","step: 9\n","step: 10\n","step: 11\n","step: 12\n","step: 13\n","step: 14\n","step: 15\n","step: 16\n","step: 17\n","step: 18\n","step: 19\n","step: 20\n","step: 21\n","step: 22\n","step: 23\n","step: 24\n","step: 25\n","step: 26\n","step: 27\n","step: 28\n","step: 29\n","step: 30\n"]}]},{"cell_type":"code","source":["! zip -r our_test.zip /content/VITON-HD/datasets/test\n","! zip -r result.zip /content/VITON-HD/results/first_test"],"metadata":{"id":"6FNa-jDp6ojS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716287502594,"user_tz":-210,"elapsed":530,"user":{"displayName":"Reza Nematollahi","userId":"07189224728977117676"}},"outputId":"d6787f31-8ffb-48f1-f4d2-9a67c2d8f2be"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/VITON-HD/datasets/test/ (stored 0%)\n","  adding: content/VITON-HD/datasets/test/image/ (stored 0%)\n","  adding: content/VITON-HD/datasets/test/image/08909_00.jpg (deflated 4%)\n","  adding: content/VITON-HD/datasets/test/image/07573_00.jpg (deflated 7%)\n","  adding: content/VITON-HD/datasets/test/image/00891_00.jpg (deflated 8%)\n","  adding: content/VITON-HD/datasets/test/image/07445_00.jpg (deflated 6%)\n","  adding: content/VITON-HD/datasets/test/image/03615_00.jpg (deflated 5%)\n","  adding: content/VITON-HD/datasets/test/image/10549_00.jpg (deflated 4%)\n","  adding: content/VITON-HD/datasets/test/cloth-mask/ (stored 0%)\n","  adding: content/VITON-HD/datasets/test/cloth-mask/06802_00.jpg (deflated 37%)\n","  adding: content/VITON-HD/datasets/test/cloth-mask/08348_00.jpg (deflated 32%)\n","  adding: content/VITON-HD/datasets/test/cloth-mask/09933_00.jpg (deflated 35%)\n","  adding: content/VITON-HD/datasets/test/cloth-mask/11351_00.jpg (deflated 37%)\n","  adding: content/VITON-HD/datasets/test/cloth-mask/07429_00.jpg (deflated 33%)\n","  adding: content/VITON-HD/datasets/test/openpose-img/ (stored 0%)\n","  adding: content/VITON-HD/datasets/test/openpose-img/10549_00_rendered.png (deflated 53%)\n","  adding: content/VITON-HD/datasets/test/openpose-img/07445_00_rendered.png (deflated 54%)\n","  adding: content/VITON-HD/datasets/test/openpose-img/07573_00_rendered.png (deflated 55%)\n","  adding: content/VITON-HD/datasets/test/openpose-img/08909_00_rendered.png (deflated 51%)\n","  adding: content/VITON-HD/datasets/test/openpose-img/03615_00_rendered.png (deflated 51%)\n","  adding: content/VITON-HD/datasets/test/openpose-img/00891_00_rendered.png (deflated 54%)\n","  adding: content/VITON-HD/datasets/test/image-parse/ (stored 0%)\n","  adding: content/VITON-HD/datasets/test/image-parse/07573_00.png (deflated 24%)\n","  adding: content/VITON-HD/datasets/test/image-parse/10549_00.png (deflated 29%)\n","  adding: content/VITON-HD/datasets/test/image-parse/08909_00.png (deflated 24%)\n","  adding: content/VITON-HD/datasets/test/image-parse/00891_00.png (deflated 29%)\n","  adding: content/VITON-HD/datasets/test/image-parse/07445_00.png (deflated 25%)\n","  adding: content/VITON-HD/datasets/test/image-parse/03615_00.png (deflated 29%)\n","  adding: content/VITON-HD/datasets/test/cloth/ (stored 0%)\n","  adding: content/VITON-HD/datasets/test/cloth/06802_00.jpg (deflated 7%)\n","  adding: content/VITON-HD/datasets/test/cloth/08348_00.jpg (deflated 4%)\n","  adding: content/VITON-HD/datasets/test/cloth/09933_00.jpg (deflated 7%)\n","  adding: content/VITON-HD/datasets/test/cloth/11351_00.jpg (deflated 10%)\n","  adding: content/VITON-HD/datasets/test/cloth/07429_00.jpg (deflated 6%)\n","  adding: content/VITON-HD/datasets/test/openpose-json/ (stored 0%)\n","  adding: content/VITON-HD/datasets/test/openpose-json/00891_00_keypoints.json (deflated 74%)\n","  adding: content/VITON-HD/datasets/test/openpose-json/08909_00_keypoints.json (deflated 75%)\n","  adding: content/VITON-HD/datasets/test/openpose-json/03615_00_keypoints.json (deflated 75%)\n","  adding: content/VITON-HD/datasets/test/openpose-json/10549_00_keypoints.json (deflated 75%)\n","  adding: content/VITON-HD/datasets/test/openpose-json/07573_00_keypoints.json (deflated 75%)\n","  adding: content/VITON-HD/datasets/test/openpose-json/07445_00_keypoints.json (deflated 75%)\n","  adding: content/VITON-HD/results/first_test/ (stored 0%)\n","  adding: content/VITON-HD/results/first_test/10549_09933_00.jpg (deflated 12%)\n","  adding: content/VITON-HD/results/first_test/08909_09933_00.jpg (deflated 12%)\n","  adding: content/VITON-HD/results/first_test/07445_11351_00.jpg (deflated 15%)\n","  adding: content/VITON-HD/results/first_test/07573_08348_00.jpg (deflated 11%)\n","  adding: content/VITON-HD/results/first_test/07445_07429_00.jpg (deflated 13%)\n","  adding: content/VITON-HD/results/first_test/03615_11351_00.jpg (deflated 11%)\n","  adding: content/VITON-HD/results/first_test/03615_07429_00.jpg (deflated 10%)\n","  adding: content/VITON-HD/results/first_test/10549_06802_00.jpg (deflated 13%)\n","  adding: content/VITON-HD/results/first_test/08909_11351_00.jpg (deflated 12%)\n","  adding: content/VITON-HD/results/first_test/07445_06802_00.jpg (deflated 14%)\n","  adding: content/VITON-HD/results/first_test/07445_09933_00.jpg (deflated 14%)\n","  adding: content/VITON-HD/results/first_test/07445_08348_00.jpg (deflated 11%)\n","  adding: content/VITON-HD/results/first_test/08909_07429_00.jpg (deflated 11%)\n","  adding: content/VITON-HD/results/first_test/00891_09933_00.jpg (deflated 14%)\n","  adding: content/VITON-HD/results/first_test/00891_07429_00.jpg (deflated 15%)\n","  adding: content/VITON-HD/results/first_test/03615_06802_00.jpg (deflated 10%)\n","  adding: content/VITON-HD/results/first_test/00891_06802_00.jpg (deflated 14%)\n","  adding: content/VITON-HD/results/first_test/00891_08348_00.jpg (deflated 11%)\n","  adding: content/VITON-HD/results/first_test/10549_08348_00.jpg (deflated 9%)\n","  adding: content/VITON-HD/results/first_test/07573_07429_00.jpg (deflated 14%)\n","  adding: content/VITON-HD/results/first_test/10549_11351_00.jpg (deflated 13%)\n","  adding: content/VITON-HD/results/first_test/03615_09933_00.jpg (deflated 10%)\n","  adding: content/VITON-HD/results/first_test/07573_06802_00.jpg (deflated 15%)\n","  adding: content/VITON-HD/results/first_test/10549_07429_00.jpg (deflated 12%)\n","  adding: content/VITON-HD/results/first_test/03615_08348_00.jpg (deflated 9%)\n","  adding: content/VITON-HD/results/first_test/07573_09933_00.jpg (deflated 16%)\n","  adding: content/VITON-HD/results/first_test/00891_11351_00.jpg (deflated 15%)\n","  adding: content/VITON-HD/results/first_test/08909_06802_00.jpg (deflated 11%)\n","  adding: content/VITON-HD/results/first_test/07573_11351_00.jpg (deflated 16%)\n","  adding: content/VITON-HD/results/first_test/08909_08348_00.jpg (deflated 10%)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"x-UFtd5tz40y"},"execution_count":null,"outputs":[]}]}